{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9d6f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d401bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-small', bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-small').cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, prompt,instruction,desired_output, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for p,i,d in zip(prompt,instruction,desired_output):\n",
    "            p = str(p)\n",
    "            i = str(i)\n",
    "            d = str(d)\n",
    "            prompt = '# '+p.lstrip('\\n') + '\\n# '+i + '\\n\\\"\\\"\\\"' + d\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + prompt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d4c8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Path to CSV')\n",
    "prompt = df['inputs']\n",
    "instruction = ['generate the future' for i in range(len(prompt))]\n",
    "target = df['targets']\n",
    "max_length = max([len(tokenizer.encode(str(t))) for t in target]) + \\\n",
    "             max([len(tokenizer.encode(str(i))) for i in instruction]) + \\\n",
    "             max([len(tokenizer.encode(str(p))) for p in prompt])\n",
    "dataset = CustomDataset(prompt,instruction,target,tokenizer,max_length)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "training_args = TrainingArguments(output_dir='./results', \n",
    "                                  num_train_epochs=4, \n",
    "                                  logging_steps=100,\n",
    "                                  save_steps=1000,\n",
    "                                  per_device_train_batch_size=1, \n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  gradient_accumulation_steps=1,\n",
    "                                  gradient_checkpointing=True,\n",
    "                                  fp16=True          #if loss return 0, change to False\n",
    "                                  optim=\"adafactor\", #change to adamw_torch if you have have enough memory['adamw_hf', 'adamw_torch', 'adamw_torch_xla', 'adamw_apex_fused', 'adafactor', 'adamw_bnb_8bit', 'sgd', 'adagrad']\n",
    "                                  warmup_steps=1, \n",
    "                                  weight_decay=0.05, \n",
    "                                  logging_dir='/home/delta/Downloads/logs', \n",
    "                                  report_to = 'tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae19e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b6fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('model')\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a0846e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Kirsten Johnson\n",
      "# generate a description for the above director\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "prompt = '''\n",
    "\n",
    "This is a decoder only model, it can generate the next word based on previous word but no way to control\n",
    "if it produce the correct or not correct answer. To control the output, you need to use an encoder and decoder model\n",
    "YOu also need to think about what type of masking you are going to use. GPT3 or T5 are just the name. The true question\n",
    "is what type of transformer architecture and what masking strategy you are going to adopt.\n",
    "\n",
    "'''\n",
    "instruction = 'generate the future'             #Same instruction as you trained\n",
    "prompt = '# '+prompt.lstrip('\\n') + '\\n# '+instruction + '\\n\\\"\\\"\\\"'\n",
    "prompt_start = prompt.rfind(instruction)+len(instruction)+4\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03874a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tragic death at an apartment complex impacts not just residents but their landlords as she navigates three husbands, one ex on the run from police harassment.\n",
      "\n",
      "In a fictional case involving a criminal and a gang at a local community college, four teen and one classmate battle a vicious and ruthless group that refuses a truce.\n",
      "\n",
      "After his daughter abruptly drops off the farm because she's taking birth-injection drugs or is dead, a mother-to-be is sent on a mad cap that gives her two extra lives.\n",
      "\n",
      "As the young lives their parents – along with all living creatures on all 4 corners of Earth, is threatened. She seeks solace with magical adventures in the small, mysterious island home.\n",
      "\n",
      "A talented but ambitious teen from Boston struggles to juggle her dreams to play soccer or fall in with a criminal cartel – but soon ends the team she falls under to.\n",
      "\n",
      "An aspiring model agrees to dance, where her talent doesn't follow when the photographer whose job she wants to get a photo sets her up for sex.\n",
      "\n",
      "A teen struggling to adjust out of her high school is transferred across campus where she'll encounter one fellow newbie — and face their shared sense of mischief for themselves.\n",
      "\n",
      "In his hometown of San Francisco in California on Christmas Eve 1947 the actor John Lithius shares candid personal insights to take stock from throughout the past and his lifetime journey.\"This poignant story explores the heartache of life after Christmas.\n",
      "\n",
      "Teen pregnancy and the emotional impact are afoot as teen sisters Hannah Fidez and Shannon take off for a honeymoon when Shannon unexpectedly reveals details and starts dating men.\n",
      "\n",
      "Mock and mirror twins Sarah, Amy Schumer returns and takes to life after returning last on a big musical hiatus, her family arrives safely in London — and the comedy is about to spill into reality.\n",
      "\n",
      "This fast-paced doc offers the intimate firsthand accounts of top musicians' performances. An intimate look at the life and death in their most coveted recording spaces.\n",
      "\n",
      "After the end a decade-long union, four women are thrown in an absurd balancing competition: the annual Woman of the Pole Competition.\n",
      "\n",
      "An aspiring reporter sets out with a notorious tycoon seeking lucrative arms to prove herself inside the global empire of terrorists.\n",
      "# generate a description for the above director\n",
      "\"\"\"After becoming suspicious of both clients for an influential radio station's CEO, she takes off on different quests.\n",
      "\n",
      "A group-disliked single mother must survive until the end when she gets hired to work as a sex worker out of fear of injury – then becomes romantically connected with her boss.\n",
      "\n",
      "An overindulged writer falls into a series of romantic choices. Based on a short film by Jeff Newman.\n",
      "\n",
      "Four very different siblings and their best man become entangled — each determined to become more ambitious in his or own lives during a tumultuous Christmas Eve getaway.\n",
      "\n",
      "After she returns from a hiatus at Amazon and meets her boyfriend’s family with her dad, Kath returns into her personal heart. So she’d better spend more times and energy on her. In their next new drama series.\n",
      "\n",
      "An unassuming college major finds joy in her relationship with a smart little kid, who brings himself into the community. If fate gives another kid their lives, she hopes is as well.\n",
      "\n",
      "Porn-loving lawyer Lana faces his future when after waking to discover that in fact her body was actually moved into his basement by his clients: Heidi or Alexis – as the former may say.\n",
      "\n",
      "On Christmas Eve, Alaska Rangers join their Alpin tribe partners to clear a path to refuge for stranded Christmas shoppers amid perilous snow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generated = tokenizer(f\"<|startoftext|>{prompt}\", return_tensors=\"pt\").input_ids.cuda()\n",
    "sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n",
    "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=20)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    output = tokenizer.decode(sample_output, skip_special_tokens=True,\n",
    "                              clean_up_tokenization_spaces=True)\n",
    "    output = output[prompt_start:]\n",
    "    print(output)\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
