{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66614919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cdcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e29bf100a3948d5b14350ecc7b3f87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b5de8fb824484f93c0ef8474cdabbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551560adaa90425c89c573271211153f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large', bos_token='<|startoftext|>',\n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-large').cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, prompt,instruction,desired_output, tokenizer, max_length):\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.labels = []\n",
    "        for p,i,d in zip(prompt,instruction,desired_output):\n",
    "            p = str(p)\n",
    "            i = str(i)\n",
    "            d = str(d)\n",
    "            prompt = '# '+p.lstrip('\\n') + '\\n# '+i + '\\n\\\"\\\"\\\"' + d\n",
    "            encodings_dict = tokenizer('<|startoftext|>' + prompt + '<|endoftext|>', truncation=True,\n",
    "                                       max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a238fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/delta/Downloads/netflix_titles.csv')\n",
    "prompt = df['director']\n",
    "instruction = ['generate a description for the above director' for i in range(len(prompt))]\n",
    "target = df['description']\n",
    "max_length = max([len(tokenizer.encode(str(t))) for t in target]) + \\\n",
    "             max([len(tokenizer.encode(str(i))) for i in instruction]) + \\\n",
    "             max([len(tokenizer.encode(str(p))) for p in prompt])\n",
    "dataset = CustomDataset(prompt,instruction,target,tokenizer,max_length)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "training_args = TrainingArguments(output_dir='./results', num_train_epochs=1, logging_steps=100, save_steps=5000,\n",
    "                                  per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "                                  warmup_steps=10, weight_decay=0.05, logging_dir='./logs', report_to = 'none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddcad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48372d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer(model=model,  args=training_args, train_dataset=train_dataset, \n",
    "        eval_dataset=val_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
    "                                                              'attention_mask': torch.stack([f[1] for f in data]),\n",
    "                                                              'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc65fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Kirsten Johnson'\n",
    "instruction = 'generate a description for the above director'\n",
    "prompt = '# '+prompt.lstrip('\\n') + '\\n# '+instruction + '\\n\\\"\\\"\\\"'\n",
    "prompt_start = prompt.rfind(instruction)+len(instruction)+4\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b04c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = tokenizer(f\"<|startoftext|>{prompt}\", return_tensors=\"pt\").input_ids.cuda()\n",
    "sample_outputs = model.generate(generated, do_sample=True, top_k=50, \n",
    "                                max_length=300, top_p=0.95, temperature=1.9, num_return_sequences=20)\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    output = tokenizer.decode(sample_output, skip_special_tokens=True,\n",
    "                              clean_up_tokenization_spaces=True)\n",
    "    output = output[prompt_start:]\n",
    "    print(output)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11580f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73771970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e7a2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3223c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
