{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parallel Transformer",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPz6ES0m7kqRXyNp6ZO9Uyv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dentadelta/123/blob/master/Parallel_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boIqxdYKmHd4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iNK8qXJmZfP",
        "outputId": "421f1a9f-26da-4862-d557-c68057533504"
      },
      "source": [
        "sentence1input = [1,2,3]\n",
        "sentence2input = [4,5,6]\n",
        "sentence1output = [7,8,9]\n",
        "sentence2output = [10,11,12]\n",
        "batch_input = [sentence1input, sentence2input]\n",
        "batch_input = torch.tensor(batch_input)\n",
        "print(batch_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbzkgNaBmaEe"
      },
      "source": [
        "h = 8 # 8 parallel attention layer. -> I probaly going to use 6 layers because a mining rig consists of 6 GPU so I'm happy with running each layer in parallel\n",
        "N = 6 # 6 stacks of encoder layer\n",
        "dmodel = 512 #512 features\n",
        "dk= int(dmodel/h) #key size, and value size\n",
        "dv = int(dmodel/h)\n",
        "dff = 2048 #embedding dimension\n",
        "maximum_sequence_length = 3 #in real world in could be something like 128 pad pad sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793oFDANozZ1",
        "outputId": "c3499f32-34fa-41f1-8cf2-b7e26c252974"
      },
      "source": [
        "num_embedding = len(sentence1input)+len(sentence2input)+len(sentence1output)+len(sentence2output)\n",
        "print('Number of vocabulary size is:', num_embedding,'\\n','In real life this number is as large as the vocabulary size - say 5000 unique words') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of vocabulary size is: 12 \n",
            " In real life this number is as large as the vocabulary size - say 5000 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_8kRU6qnWP7"
      },
      "source": [
        "# lets create an embedded layer:\n",
        "embedded_layer = nn.Embedding(dff,dmodel).to('cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpzn_Zvco6uB"
      },
      "source": [
        "# lets transform the batch input\n",
        "batch_input = embedded_layer(batch_input.to('cuda:0'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOSymfZ1pLjW"
      },
      "source": [
        "#lets apply positional encoding --> Borrow this function from Pytorch (because it works)\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKXpT7Doqbik"
      },
      "source": [
        "pe = PositionalEncoding(dmodel,0.1,num_embedding).to('cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuDM2QsdrbCv"
      },
      "source": [
        "batch_input = pe(batch_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcwyMLo49RS"
      },
      "source": [
        "# Lets create a self attention layer\n",
        "class MultiAttentionHead(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,maximum_sequence_length):\n",
        "    super(MultiAttentionHead, self).__init__()\n",
        "\n",
        "    # This is where you split your model, knowing that everything will be converted to CPU for concat operation then put back the output to GPU:0\n",
        "    # You can only use 1 GPU in google colab, thats why I only use GPU:0\n",
        "    # The vanila transformer paper states that a mask layer is only an optional in an encoder layer, thats why I havent implemented\n",
        "\n",
        "    #Layer1: GPU:0\n",
        "    self.keyTransformer_1 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_1 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_1  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer2: GPU:1\n",
        "    self.keyTransformer_2 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_2 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_2  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer3: GPU:2\n",
        "    self.keyTransformer_3 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_3 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_3  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer4: GPU:3\n",
        "    self.keyTransformer_4 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_4 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_4  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer5: GPU:4\n",
        "    self.keyTransformer_5 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_5 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_5  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer6: GPU:5\n",
        "    self.keyTransformer_6 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_6 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_6 = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer7: GPU:6\n",
        "    self.keyTransformer_7 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_7 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_7  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer8: GPU:7\n",
        "    self.keyTransformer_8 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_8 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_8  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "\n",
        "    #Weight_0 layer:\n",
        "    self.W0 = nn.Linear(dmodel,dmodel).to('cuda:0')   #Size h*dv x dmodel. But since dv = dk and dk x h = dv so it's a dmodel x dmodel layer -> cuda:0\n",
        "\n",
        "\n",
        "    #LayerNormalisation\n",
        "    self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1).to('cuda:0')\n",
        "  \n",
        "\n",
        "  def forward(self,x):\n",
        "    k1 = self.keyTransformer_1(x)\n",
        "    q1 = self.querryTransformer_1(x)\n",
        "    v1 = self.valueTrasformer_1(x)\n",
        "\n",
        "    x.to('cuda:0')  #Replace cuda:0 with the correct GPU you chose for layer 2\n",
        "\n",
        "    k2 = self.keyTransformer_2(x)\n",
        "    q2 = self.querryTransformer_2(x)\n",
        "    v2 = self.valueTrasformer_2(x)\n",
        "\n",
        "    x.to('cuda:0'). #Replace cuda:0 with the correct GPU you chose for layer 3\n",
        "\n",
        "    k3 = self.keyTransformer_3(x)\n",
        "    q3 = self.querryTransformer_3(x)\n",
        "    v3 = self.valueTrasformer_3(x)\n",
        "\n",
        "    x.to('cuda:0')  #Replace cuda:0 with the correct GPU you chose for layer 4\n",
        "\n",
        "    k4 = self.keyTransformer_4(x)\n",
        "    q4 = self.querryTransformer_4(x)\n",
        "    v4 = self.valueTrasformer_4(x)\n",
        "\n",
        "    x.to('cuda:0'). #Replace cuda:0 with the correct GPU you chose for layer 5\n",
        "\n",
        "    k5 = self.keyTransformer_5(x)\n",
        "    q5 = self.querryTransformer_5(x)\n",
        "    v5 = self.valueTrasformer_5(x)\n",
        "\n",
        "    x.to('cuda:0'). #Replace cuda:0 with the correct GPU you chose for layer 6\n",
        "\n",
        "    k6 = self.keyTransformer_6(x)\n",
        "    q6 = self.querryTransformer_6(x)\n",
        "    v6 = self.valueTrasformer_6(x)\n",
        "\n",
        "    x.to('cuda:0'). #Replace cuda:0 with the correct GPU you chose for layer 7\n",
        "\n",
        "    k7 = self.keyTransformer_7(x)\n",
        "    q7 = self.querryTransformer_7(x)\n",
        "    v7 = self.valueTrasformer_7(x)\n",
        "\n",
        "    x.to('cuda:0'). #Replace cuda:0 with the correct GPU you chose for layer 8\n",
        "\n",
        "    k8 = self.keyTransformer_8(x)\n",
        "    q8 = self.querryTransformer_8(x)\n",
        "    v8 = self.valueTrasformer_8(x)\n",
        "\n",
        "    \n",
        "    def calculate_head(q,k,v):\n",
        "      return torch.matmul(F.softmax(torch.matmul(q,k.transpose(-2,-1))/(dk**0.5)),v)\n",
        "\n",
        "    head_1 = calculate_head(q1,k1,v1) \n",
        "    head_1.to('cpu')                  \n",
        "\n",
        "    head_2 = calculate_head(q2,k2,v2) \n",
        "    head_2.to('cpu')                  \n",
        "\n",
        "    head_3 = calculate_head(q3,k3,v3) \n",
        "    head_3.to('cpu')                  \n",
        "\n",
        "    head_4 = calculate_head(q4,k4,v4) \n",
        "    head_4.to('cpu')                  \n",
        "\n",
        "    head_5 = calculate_head(q1,k1,v1) \n",
        "    head_5.to('cpu')                  \n",
        "\n",
        "    head_6 = calculate_head(q2,k2,v2) \n",
        "    head_6.to('cpu')                  \n",
        "\n",
        "    head_7 = calculate_head(q3,k3,v3) \n",
        "    head_7.to('cpu')                  \n",
        "\n",
        "    head_8 = calculate_head(q4,k4,v4) \n",
        "    head_8.to('cpu')                   \n",
        "\n",
        "\n",
        "    All_heads = torch.cat((head_1,head_2,head_3,head_4,head_5,head_6,head_7,head_8), dim=2) #in cpu\n",
        "\n",
        "    All_heads.to('cuda:0') #Transfer back to Cuda:0 for matrix operation\n",
        "\n",
        "    All_heads = self.W0(All_heads) # cuda:0\n",
        "\n",
        "    x.to('cuda:0') # transfer back to cuda:0 for layer normalisation operation\n",
        "\n",
        "    All_heads = self.Add_and_Nom(x + All_heads)  #cuda:0\n",
        "\n",
        "    All_heads = self.dropout(All_heads) #cuda:0\n",
        "\n",
        "    return All_heads #cuda:0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuC01HueKmAZ"
      },
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,maximum_sequence_length,dff):\n",
        "    super(FFN,self).__init__()\n",
        "    # 6 stacks in total\n",
        "    self.multihead_1 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_2 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_3 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_4 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_5 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_6 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "\n",
        "    self.lin1a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout1 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin1b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin2a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout2 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin2b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin3a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout3 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin3b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin4a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout4 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin4b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin5a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout5 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin5b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin6a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout6 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin6b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    sublayer1 = self.multihead_1(x)\n",
        "    sublayer1 = self.lin1a(sublayer1)\n",
        "    sublayer1 = F.relu(sublayer1)\n",
        "    sublayer1 = self.dropout1(sublayer1)\n",
        "    sublayer1 = self.lin1b(sublayer1)\n",
        "    x = sublayer1\n",
        "\n",
        "    sublayer2 = self.multihead_2(x)\n",
        "    sublayer2 = self.lin2a(sublayer2)\n",
        "    sublayer2 = F.relu(sublayer2)\n",
        "    sublayer2 = self.dropout2(sublayer2)\n",
        "    sublayer2 = self.lin2b(sublayer2)\n",
        "    x = sublayer2\n",
        "\n",
        "    sublayer3 = self.multihead_3(x)\n",
        "    sublayer3 = self.lin3a(sublayer3)\n",
        "    sublayer3 = F.relu(sublayer3)\n",
        "    sublayer3 = self.dropout3(sublayer3)\n",
        "    sublayer3 = self.lin3b(sublayer3)\n",
        "    x = sublayer3\n",
        "\n",
        "    sublayer4 = self.multihead_4(x)\n",
        "    sublayer4 = self.lin4a(sublayer4)\n",
        "    sublayer4 = F.relu(sublayer4)\n",
        "    sublayer4 = self.dropout4(sublayer4)\n",
        "    sublayer4 = self.lin4b(sublayer4)\n",
        "    x = sublayer4\n",
        "\n",
        "    sublayer5 = self.multihead_5(x)\n",
        "    sublayer5 = self.lin5a(sublayer5)\n",
        "    sublayer5 = F.relu(sublayer5)\n",
        "    sublayer5 = self.dropout5(sublayer5)\n",
        "    sublayer5 = self.lin5b(sublayer5)\n",
        "    x = sublayer5\n",
        "\n",
        "    sublayer6 = self.multihead_6(x)\n",
        "    sublayer6 = self.lin6a(sublayer6)\n",
        "    sublayer6 = F.relu(sublayer6)\n",
        "    sublayer6 = self.dropout6(sublayer6)\n",
        "    sublayer6 = self.lin6b(sublayer6)\n",
        "\n",
        "    \n",
        "\n",
        "    return sublayer6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8FXb9QoNHFR"
      },
      "source": [
        "LastEncoderLayer = FFN(dmodel,dk,dv,maximum_sequence_length,dff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oibylKOnlWD2",
        "outputId": "a9a08420-2df4-4294-894f-c5adbd49ca90"
      },
      "source": [
        "LastEncoderLayer(batch_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3714,  0.5405, -0.2181,  ...,  0.4281, -0.1826, -0.0062],\n",
              "         [-0.0383,  0.2671, -0.4236,  ...,  0.0989, -0.2363, -0.1850],\n",
              "         [ 0.5845,  0.3287, -0.2716,  ...,  0.1759, -0.0156,  0.0555]],\n",
              "\n",
              "        [[ 0.2852,  0.5044, -0.4051,  ...,  0.1559,  0.0020,  0.0632],\n",
              "         [-0.0102,  0.1107, -0.3334,  ...,  0.0876,  0.0242,  0.1760],\n",
              "         [ 0.2235,  0.1331, -0.0245,  ..., -0.0125,  0.0559, -0.1872]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0a5h7lSmRte"
      },
      "source": [
        "# Still need to make a final encoder layer to wrap everything together\n",
        "# Then work on the decoder part (which is the same as an encoder with a layer mask + using key and querry from the encoder...\n",
        "# Also need to add a softmax layer at the end of Decoder output to get the actual next token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXWpswyZ4WwU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}