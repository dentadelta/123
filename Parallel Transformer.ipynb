{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuRnvMGWzsZo4JgopWkd61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dentadelta/123/blob/master/Parallel%20Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boIqxdYKmHd4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iNK8qXJmZfP",
        "outputId": "3f8d55e8-29d7-4ec4-9835-8d42193f1ec4"
      },
      "source": [
        "sentence1input = [1,2,3]\n",
        "sentence2input = [4,5,6]\n",
        "sentence1output = [7,8,9]\n",
        "sentence2output = [10,11,12]\n",
        "batch_input = [sentence1input, sentence2input]\n",
        "batch_input = torch.tensor(batch_input)\n",
        "print(batch_input)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbzkgNaBmaEe"
      },
      "source": [
        "h = 8 # 8 parallel attention layer\n",
        "N = 6 # 6 stacks of encoder layer\n",
        "dmodel = 512 #512 features\n",
        "dk= int(dmodel/h) #key size, and value size\n",
        "dv = int(dmodel/h)\n",
        "dff = 2048 #embedding dimension\n",
        "maximum_sequence_length = 3 #in real world in could be something like 128 pad pad sequence"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793oFDANozZ1",
        "outputId": "bd613edd-d68f-4a67-edbe-036ab6552bc2"
      },
      "source": [
        "num_embedding = len(sentence1input)+len(sentence2input)+len(sentence1output)+len(sentence2output)\n",
        "print('Number of vocabulary size is:', num_embedding,'\\n','In real life this number is as large as the vocabulary size - say 5000 unique words') "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of vocabulary size is: 12 \n",
            " In real life this number is as large as the vocabulary size - say 5000 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_8kRU6qnWP7"
      },
      "source": [
        "# lets create an embedded layer:\n",
        "embedded_layer = nn.Embedding(dff,dmodel).to('cuda')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpzn_Zvco6uB"
      },
      "source": [
        "# lets transform the batch input\n",
        "batch_input = embedded_layer(batch_input.to('cuda'))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOSymfZ1pLjW"
      },
      "source": [
        "#lets apply positional encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKXpT7Doqbik"
      },
      "source": [
        "pe = PositionalEncoding(512,0.1,12).to('cuda')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuDM2QsdrbCv"
      },
      "source": [
        "batch_input = pe(batch_input)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcwyMLo49RS"
      },
      "source": [
        "# Lets create a self attention layer\n",
        "class MultiAttentionHead(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,maximum_sequence_length):\n",
        "    super(MultiAttentionHead, self).__init__()\n",
        "\n",
        "    # I have two GPUs\n",
        "    #Layer1: GPU:0\n",
        "    self.keyTransformer_1 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_1 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_1  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer2: GPU:0\n",
        "    self.keyTransformer_2 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_2 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_2  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer3: GPU:0\n",
        "    self.keyTransformer_3 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_3 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_3  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer4: GPU:1\n",
        "    self.keyTransformer_4 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_4 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_4  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer5: GPU:1\n",
        "    self.keyTransformer_5 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_5 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_5  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer6: GPU:1\n",
        "    self.keyTransformer_6 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_6 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_6 = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer7: GPU:1\n",
        "    self.keyTransformer_7 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_7 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_7  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer8: GPU:1\n",
        "    self.keyTransformer_8 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_8 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_8  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "\n",
        "    #Weight_0 layer:\n",
        "    self.W0 = nn.Linear(dmodel,dmodel).to('cuda:0')   #Size h*dv x dmodel. But since dv = dk and dk x h = dv so it's a dmodel x dmodel layer -> cuda:0\n",
        "\n",
        "\n",
        "    #LayerNormalisation\n",
        "    self.Add_and_Nom = nn.LayerNorm(dmodel, eps=1e-05, elementwise_affine=True).to('cuda:0')\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1).to('cuda:0')\n",
        "  \n",
        "\n",
        "  def forward(self,x):\n",
        "    k1 = self.keyTransformer_1(x)\n",
        "    q1 = self.querryTransformer_1(x)\n",
        "    v1 = self.valueTrasformer_1(x)\n",
        "\n",
        "    k2 = self.keyTransformer_2(x)\n",
        "    q2 = self.querryTransformer_2(x)\n",
        "    v2 = self.valueTrasformer_2(x)\n",
        "\n",
        "    k3 = self.keyTransformer_3(x)\n",
        "    q3 = self.querryTransformer_3(x)\n",
        "    v3 = self.valueTrasformer_3(x)\n",
        "\n",
        "    k4 = self.keyTransformer_4(x)\n",
        "    q4 = self.querryTransformer_4(x)\n",
        "    v4 = self.valueTrasformer_4(x)\n",
        "\n",
        "    x.to('cuda:0')\n",
        "\n",
        "    k5 = self.keyTransformer_5(x)\n",
        "    q5 = self.querryTransformer_5(x)\n",
        "    v5 = self.valueTrasformer_5(x)\n",
        "\n",
        "    k6 = self.keyTransformer_6(x)\n",
        "    q6 = self.querryTransformer_6(x)\n",
        "    v6 = self.valueTrasformer_6(x)\n",
        "\n",
        "    k7 = self.keyTransformer_7(x)\n",
        "    q7 = self.querryTransformer_7(x)\n",
        "    v7 = self.valueTrasformer_7(x)\n",
        "\n",
        "    k8 = self.keyTransformer_8(x)\n",
        "    q8 = self.querryTransformer_8(x)\n",
        "    v8 = self.valueTrasformer_8(x)\n",
        "    def calculate_head(q,k,v):\n",
        "      return torch.matmul(F.softmax(torch.matmul(q,k.transpose(-2,-1))/(dk**0.5)),v)\n",
        "\n",
        "    head_1 = calculate_head(q1,k1,v1) #cuda:0\n",
        "    head_1.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_2 = calculate_head(q2,k2,v2) #cuda:0\n",
        "    head_2.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_3 = calculate_head(q3,k3,v3) #cuda:0\n",
        "    head_3.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_4 = calculate_head(q4,k4,v4) #cuda:0\n",
        "    head_4.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_5 = calculate_head(q1,k1,v1) #cuda:1\n",
        "    head_5.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_6 = calculate_head(q2,k2,v2) #cuda:1\n",
        "    head_6.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_7 = calculate_head(q3,k3,v3) #cuda:1\n",
        "    head_7.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_8 = calculate_head(q4,k4,v4) #cuda:1\n",
        "    head_8.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU   \n",
        "\n",
        "\n",
        "    All_heads = torch.cat((head_1,head_2,head_3,head_4,head_5,head_6,head_7,head_8), dim=2) #in cpu\n",
        "\n",
        "    All_heads.to('cuda:0') #Transfer back to Cuda:0 for matrix operation\n",
        "\n",
        "    All_heads = self.W0(All_heads) # cuda:0\n",
        "\n",
        "    x.to('cuda:0') # transfer back to cuda:0 for layer normalisation operation\n",
        "\n",
        "    All_heads = self.Add_and_Nom(x + All_heads)  #cuda:0\n",
        "\n",
        "    All_heads = self.dropout(All_heads) #cuda:0\n",
        "\n",
        "    return All_heads #cuda:0"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuC01HueKmAZ"
      },
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,maximum_sequence_length,dff):\n",
        "    super(FFN,self).__init__()\n",
        "    # 6 stacks in total\n",
        "    self.multihead_1 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_2 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_3 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_4 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_5 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_6 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "\n",
        "    self.lin1a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout1 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin1b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin2a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout2 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin2b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin3a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout3 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin3b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin4a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout4 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin4b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin5a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout5 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin5b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "    self.lin6a = nn.Linear(dmodel,dff).to('cuda:0')\n",
        "    self.dropout6 = nn.Dropout(0.1).to('cuda:0')\n",
        "    self.lin6b = nn.Linear(dff,dmodel).to('cuda:0')\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    sublayer1 = self.multihead_1(x)\n",
        "    sublayer1 = self.lin1a(sublayer1)\n",
        "    sublayer1 = F.relu(sublayer1)\n",
        "    sublayer1 = self.dropout1(sublayer1)\n",
        "    sublayer1 = self.lin1b(sublayer1)\n",
        "    x = sublayer1\n",
        "\n",
        "    sublayer2 = self.multihead_2(x)\n",
        "    sublayer2 = self.lin2a(sublayer2)\n",
        "    sublayer2 = F.relu(sublayer2)\n",
        "    sublayer2 = self.dropout2(sublayer2)\n",
        "    sublayer2 = self.lin2b(sublayer2)\n",
        "    x = sublayer2\n",
        "\n",
        "    sublayer3 = self.multihead_3(x)\n",
        "    sublayer3 = self.lin3a(sublayer3)\n",
        "    sublayer3 = F.relu(sublayer3)\n",
        "    sublayer3 = self.dropout3(sublayer3)\n",
        "    sublayer3 = self.lin3b(sublayer3)\n",
        "    x = sublayer3\n",
        "\n",
        "    sublayer4 = self.multihead_4(x)\n",
        "    sublayer4 = self.lin4a(sublayer4)\n",
        "    sublayer4 = F.relu(sublayer4)\n",
        "    sublayer4 = self.dropout4(sublayer4)\n",
        "    sublayer4 = self.lin4b(sublayer4)\n",
        "    x = sublayer4\n",
        "\n",
        "    sublayer5 = self.multihead_5(x)\n",
        "    sublayer5 = self.lin5a(sublayer5)\n",
        "    sublayer5 = F.relu(sublayer5)\n",
        "    sublayer5 = self.dropout5(sublayer5)\n",
        "    sublayer5 = self.lin5b(sublayer5)\n",
        "    x = sublayer5\n",
        "\n",
        "    sublayer6 = self.multihead_6(x)\n",
        "    sublayer6 = self.lin6a(sublayer6)\n",
        "    sublayer6 = F.relu(sublayer6)\n",
        "    sublayer6 = self.dropout6(sublayer6)\n",
        "    sublayer6 = self.lin6b(sublayer6)\n",
        "\n",
        "    \n",
        "\n",
        "    return sublayer6"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8FXb9QoNHFR"
      },
      "source": [
        "encoder = FFN(dmodel,dk,dv,maximum_sequence_length,dff)"
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oibylKOnlWD2",
        "outputId": "5cd7354a-7c2e-4fd6-8b8d-42f9d3b6ff14"
      },
      "source": [
        "encoder(batch_input)"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.2917,  0.1028, -0.2736,  ...,  0.2624,  0.0885,  0.1086],\n",
              "         [ 0.0789,  0.2229, -0.3200,  ...,  0.4262,  0.1230,  0.0689],\n",
              "         [-0.0215, -0.0234, -0.3663,  ..., -0.0704, -0.3016, -0.2177]],\n",
              "\n",
              "        [[ 0.1508,  0.1198, -0.3711,  ...,  0.2681, -0.0459,  0.1767],\n",
              "         [ 0.1680,  0.3503, -0.2481,  ...,  0.2451,  0.0123,  0.0520],\n",
              "         [-0.0484,  0.1794, -0.0436,  ...,  0.1277, -0.1004,  0.1480]]],\n",
              "       device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0a5h7lSmRte"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}