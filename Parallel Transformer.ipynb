{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTHQcFOOY7XldUArX1/Xsf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dentadelta/123/blob/master/Parallel%20Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boIqxdYKmHd4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iNK8qXJmZfP",
        "outputId": "996f1815-7e13-464b-9110-4cf43bee9415"
      },
      "source": [
        "sentence1input = [1,2,3]\n",
        "sentence2input = [4,5,6]\n",
        "sentence1output = [7,8,9]\n",
        "sentence2output = [10,11,12]\n",
        "batch_input = [sentence1input, sentence2input]\n",
        "batch_input = torch.tensor(batch_input)\n",
        "print(batch_input)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbzkgNaBmaEe"
      },
      "source": [
        "h = 2 # 8 parallel attention layer\n",
        "N = 6 # 6 stacks of encoder layer\n",
        "dmodel = 512 #512 features\n",
        "dk= int(dmodel/h) #key size, and value size\n",
        "dv = int(dmodel/h)\n",
        "dff = N*dmodel #embedding dimension\n",
        "maximum_sequence_length = 3 #in real world in could be something like 128 pad pad sequence"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793oFDANozZ1",
        "outputId": "fd202da6-8f67-4179-f2a9-c912092c472b"
      },
      "source": [
        "num_embedding = len(sentence1input)+len(sentence2input)+len(sentence1output)+len(sentence2output)\n",
        "print('Number of vocabulary size is:', num_embedding,'\\n','In real life this number is as large as the vocabulary size - say 5000 unique words') "
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of vocabulary size is: 12 \n",
            " In real life this number is as large as the vocabulary size - say 5000 unique words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_8kRU6qnWP7"
      },
      "source": [
        "# lets create an embedded layer:\n",
        "embedded_layer = nn.Embedding(dff,dmodel)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpzn_Zvco6uB"
      },
      "source": [
        "# lets transform the batch input\n",
        "batch_input = embedded_layer(batch_input)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOSymfZ1pLjW"
      },
      "source": [
        "#lets apply positional encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKXpT7Doqbik"
      },
      "source": [
        "pe = PositionalEncoding(512,0.1,12)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuDM2QsdrbCv"
      },
      "source": [
        "batch_input = pe(batch_input)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TcwyMLo49RS"
      },
      "source": [
        "# Lets create a self attention layer\n",
        "class MultiAttentionHead(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,maximum_sequence_length):\n",
        "    super(MultiAttentionHead, self).__init__()\n",
        "\n",
        "    # I have two GPUs\n",
        "    #Layer1: GPU:0\n",
        "    self.keyTransformer_1 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_1 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_1  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer2: GPU:0\n",
        "    self.keyTransformer_2 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_2 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_2  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer3: GPU:0\n",
        "    self.keyTransformer_3 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_3 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_3  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer4: GPU:1\n",
        "    self.keyTransformer_4 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.querryTransformer_4 = nn.Linear(dmodel,dk).to('cuda:0')\n",
        "    self.valueTrasformer_4  = nn.Linear(dmodel,dv).to('cuda:0')\n",
        "\n",
        "\n",
        "    #Layer5: GPU:1\n",
        "    self.keyTransformer_5 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.querryTransformer_5 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.valueTrasformer_5  = nn.Linear(dmodel,dv).to('cuda:1')\n",
        "\n",
        "\n",
        "    #Layer6: GPU:1\n",
        "    self.keyTransformer_6 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.querryTransformer_6 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.valueTrasformer_6 = nn.Linear(dmodel,dv).to('cuda:1')\n",
        "\n",
        "\n",
        "    #Layer7: GPU:1\n",
        "    self.keyTransformer_7 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.querryTransformer_7 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.valueTrasformer_7  = nn.Linear(dmodel,dv).to('cuda:1')\n",
        "\n",
        "\n",
        "    #Layer8: GPU:1\n",
        "    self.keyTransformer_8 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.querryTransformer_8 = nn.Linear(dmodel,dk).to('cuda:1')\n",
        "    self.valueTrasformer_8  = nn.Linear(dmodel,dv).to('cuda:1')\n",
        "\n",
        "\n",
        "\n",
        "    #Weight_0 layer:\n",
        "    self.W0 = nn.Linear(dmodel,dmodel).to('cuda:0')   #Size h*dv x dmodel. But since dv = dk and dk x h = dv so it's a dmodel x dmodel layer -> cuda:0\n",
        "\n",
        "\n",
        "    #LayerNormalisation\n",
        "    self.Add_and_Nom = nn.LayerNorm((maximum_sequence_length,dmodel), eps=1e-05, elementwise_affine=True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    k1 = self.keyTransformer_1(x)\n",
        "    q1 = self.querryTransformer_1(x)\n",
        "    v1 = self.valueTrasformer_1(x)\n",
        "\n",
        "    k2 = self.keyTransformer_2(x)\n",
        "    q2 = self.querryTransformer_2(x)\n",
        "    v2 = self.valueTrasformer_2(x)\n",
        "\n",
        "    k3 = self.keyTransformer_3(x)\n",
        "    q3 = self.querryTransformer_3(x)\n",
        "    v3 = self.valueTrasformer_3(x)\n",
        "\n",
        "    k4 = self.keyTransformer_4(x)\n",
        "    q4 = self.querryTransformer_4(x)\n",
        "    v4 = self.valueTrasformer_4(x)\n",
        "\n",
        "    x.to('cuda:1')\n",
        "\n",
        "    k5 = self.keyTransformer_5(x)\n",
        "    q5 = self.querryTransformer_5(x)\n",
        "    v5 = self.valueTrasformer_5(x)\n",
        "\n",
        "    k6 = self.keyTransformer_6(x)\n",
        "    q6 = self.querryTransformer_6(x)\n",
        "    v6 = self.valueTrasformer_6(x)\n",
        "\n",
        "    k7 = self.keyTransformer_7(x)\n",
        "    q7 = self.querryTransformer_7(x)\n",
        "    v7 = self.valueTrasformer_7(x)\n",
        "\n",
        "    k8 = self.keyTransformer_8(x)\n",
        "    q8 = self.querryTransformer_8(x)\n",
        "    v8 = self.valueTrasformer_8(x)\n",
        "\n",
        "\n",
        "    head_1 = calculate_head(q1,k1,v1) #cuda:0\n",
        "    head_1.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_2 = calculate_head(q2,k2,v2) #cuda:0\n",
        "    head_2.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_3 = calculate_head(q3,k3,v3) #cuda:0\n",
        "    head_3.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_4 = calculate_head(q4,k4,v4) #cuda:0\n",
        "    head_4.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_5 = calculate_head(q1,k1,v1) #cuda:1\n",
        "    head_5.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_6 = calculate_head(q2,k2,v2) #cuda:1\n",
        "    head_6.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_7 = calculate_head(q3,k3,v3) #cuda:1\n",
        "    head_7.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU\n",
        "\n",
        "    head_8 = calculate_head(q4,k4,v4) #cuda:1\n",
        "    head_8.to('cpu')                  # transfer everything back to cpu because torch.cat is faster on cpu to reduce model size on GPU   \n",
        "\n",
        "\n",
        "    All_heads = torch.cat((head_1,head_2,head_3,head_4,head_5,head_6,head_7,head_8), dim=2) #in cpu\n",
        "\n",
        "    All_heads.to('cuda:0') #Transfer back to Cuda:0 for matrix operation\n",
        "\n",
        "    All_heads = self.W0(All_heads) # cuda:0\n",
        "\n",
        "    x.to('cuda:0') # transfer back to cuda:0 for layer normalisation operation\n",
        "\n",
        "    All_heads = self.Add_and_Nom(x + All_heads)  #cuda:0\n",
        "\n",
        "    return All_heads #cuda:0\n",
        "\n",
        "  def calculate_head(self,q,k,v):\n",
        "    return torch.matmul(F.softmax(torch.matmul(q,k.transpose(-2,-1))/(dk**0.5)),v)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuC01HueKmAZ"
      },
      "source": [
        "class FFN(nn.Module):\n",
        "  def __init__(self,dmodel,dk,dv,maximum_sequence_length,dff):\n",
        "    super(FFN,self).__init__()\n",
        "    # 6 stacks in total\n",
        "    self.multihead_1 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_2 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_3 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_4 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_5 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "    self.multihead_6 = MultiAttentionHead(dmodel,dk,dv,maximum_sequence_length)\n",
        "\n",
        "    self.FFN1 = nn.Linear(dmodel,dff)\n",
        "    self.FFN2 = nn.Linear(dmodel,dff)\n",
        "    self.FFN3 = nn.Linear(dmodel,dff)\n",
        "    self.FFN4 = nn.Linear(dmodel,dff)\n",
        "    self.FFN5 = nn.Linear(dmodel,dff)\n",
        "    self.FFN6 = nn.Linear(dmodel,dff)\n",
        "\n",
        "    self.add_and_norm_1 = nn.LayerNorm((dff,dmodel), eps=1e-05, elementwise_affine=True)\n",
        "    self.add_and_norm_2 = nn.LayerNorm((dff,dmodel), eps=1e-05, elementwise_affine=True)\n",
        "    self.add_and_norm_3 = nn.LayerNorm((dff,dmodel), eps=1e-05, elementwise_affine=True)\n",
        "    self.add_and_norm_4 = nn.LayerNorm((dff,dmodel), eps=1e-05, elementwise_affine=True)\n",
        "    self.add_and_norm_5 = nn.LayerNorm((dff,dmodel), eps=1e-05, elementwise_affine=True)\n",
        "    self.add_and_norm_6 = nn.LayerNorm((dff,dmodel), eps=1e-05, elementwise_affine=True)\n",
        "\n",
        "  def forward(self,x):\n",
        "    sublayer1 = self.multihead_1(x)\n",
        "    sublayer1 = self.FFN1(sublayer1)\n",
        "    sublayer1 = F.relu(sublayer1)\n",
        "    sublayer1 = self.add_and_norm_1(x+sublayer1)\n",
        "\n",
        "    sublayer2 = self.multihead_2(sublayer1)\n",
        "    sublayer2 = self.FFN2(sublayer2)\n",
        "    sublayer2 = F.relu(sublayer2)\n",
        "    sublayer2 = self.add_and_norm_2(sublayer1+sublayer2)\n",
        "\n",
        "    sublayer3 = self.multihead_3(sublayer2)\n",
        "    sublayer3 = self.FFN3(sublayer3)\n",
        "    sublayer3 = F.relu(sublayer3)\n",
        "    sublayer3 = self.add_and_norm_3(sublayer2+sublayer3)\n",
        "\n",
        "    sublayer4 = self.multihead_4(sublayer3)\n",
        "    sublayer4 = self.FFN4(sublayer4)\n",
        "    sublayer4 = F.relu(sublayer4)\n",
        "    sublayer4 = self.add_and_norm_4(sublayer3+sublayer4)\n",
        "\n",
        "    sublayer5 = self.multihead_5(sublayer4)\n",
        "    sublayer5 = self.FFN5(sublayer5)\n",
        "    sublayer5 = F.relu(sublayer5)\n",
        "    sublayer5 = self.add_and_norm_5(sublayer4+sublayer5)\n",
        "\n",
        "    sublayer6 = self.multihead_6(sublayer5)\n",
        "    sublayer6 = self.FFN6(sublayer6)\n",
        "    sublayer6 = F.relu(sublayer6)\n",
        "    sublayer6 = self.add_and_norm_6(sublayer5+sublayer6)\n",
        "\n",
        "    return sublayer6"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8FXb9QoNHFR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
