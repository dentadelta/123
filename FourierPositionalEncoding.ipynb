{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2feeff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from einops import rearrange, reduce\n",
    "import torch\n",
    "import math\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e271f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_Size = 5\n",
    "Image_Height = 224\n",
    "Image_Width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa41a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Create a fake batch image. Will use actual images later to see if the network can learn.For now, lets create the model first\n",
    "batch_image = torch.rand(Batch_Size,3,Image_Height,Image_Width)\n",
    "print(batch_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc8551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise the image:\n",
    "mean, std = batch_image.mean([2,3]), batch_image.std([2,3])\n",
    "i = 0\n",
    "for m,s in zip(mean,std):\n",
    "    normaliser = transforms.Normalize(m, s)\n",
    "    normalised_image = normaliser(batch_image[i])\n",
    "    scale = max(abs(normalised_image.max()),abs(normalised_image.min()))\n",
    "    batch_image[i] = normalised_image/scale\n",
    "    i += 1\n",
    "    \n",
    "batch_image = reduce(batch_image,'b c h w -> b h w', 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd74afb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_encoder(batch_image,K=7):\n",
    "    m = 256    \n",
    "    pe = torch.rand(batch_image.size(0),batch_image.size(1),batch_image.size(2),4*K + 1) \n",
    "    band_frequency = torch.logspace(start=1, end= m/2,steps=K,base=2,dtype=torch.float64)\n",
    "    x_normalised_coordinate = torch.linspace(start=-1, end=1,steps=batch_image.size(1),dtype=torch.float64)\n",
    "    y_normalised_coordinate = torch.linspace(start=-1, end=1,steps=batch_image.size(2),dtype=torch.float64)\n",
    "    b = 0\n",
    "    for b in range(batch_image.size(0)):\n",
    "        x = 0\n",
    "        for i in x_normalised_coordinate:\n",
    "            angle_x = i*math.pi*band_frequency\n",
    "            sin_x = torch.sin(angle_x)\n",
    "            cos_x = torch.cos(angle_x)\n",
    "            y = 0\n",
    "            for j in y_normalised_coordinate:\n",
    "                pixel_value = batch_image[b][x][y]\n",
    "                angle_y = j*math.pi*band_frequency\n",
    "                sin_y = torch.sin(angle_y)\n",
    "                cos_y = torch.cos(angle_y)\n",
    "                for k in range(K):\n",
    "                    pe[b][x][y][k*2] = sin_x[k]\n",
    "                    pe[b][x][y][k*2 + 1] = cos_x[0]\n",
    "                    pe[b][x][y][k*2 + 2*K] = sin_y[k]\n",
    "                    pe[b][x][y][k*2 + 2*K + 1] = cos_y[k]\n",
    "                pe[b][x][y][-1] = pixel_value\n",
    "                y += 1\n",
    "            x += 1\n",
    "    return rearrange(pe,'b h w c -> b (h w) c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e4c5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = fourier_encoder(batch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7af3f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_array = torch.rand(5,32,32)\n",
    "latent_pe = fourier_encoder(latent_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3233c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_q = torch.nn.Linear(29,512)\n",
    "to_k = torch.nn.Linear(29,512)\n",
    "to_v = torch.nn.Linear(29,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c067993",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = to_q(latent_pe)\n",
    "k = to_k(pe)\n",
    "v = to_v(pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75bf9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = torch.einsum('b i d , b j d -> b i j', q, k)/512**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47caab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.nn.functional.softmax(I, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f31b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = torch.einsum('b i j , b j d -> b i d', weight, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1df11ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
